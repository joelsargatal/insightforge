import os
import openai
from dotenv import load_dotenv
from agent.rag_chat import qa_chain
from langchain.evaluation.qa import QAEvalChain
from langchain_openai import ChatOpenAI
from evaluation.qa_eval_sets import qa_sets
import streamlit as st
import random

# Load environment variables from .env file
load_dotenv(dotenv_path="/Users/scarbez-ai/Documents/Projects/_env/keys.env")

# Set the OpenAI key
openai.api_key = os.getenv("OPENAI_API_KEY")

# Setup LLM and evaluation chain
llm = ChatOpenAI(model="gpt-4o")
eval_chain = QAEvalChain.from_llm(llm)

def initialize_predictions(examples):
    for i, ex in enumerate(examples):
        if not ex.get("prediction"):
            print(f"ğŸ” Generating prediction for Q{i+1}: {ex['query']}")
            st.markdown(f"ğŸ” Generating prediction for Q{i+1}: {ex['query']}")
            try:
                result = qa_chain.invoke({"question": ex["query"]})
                ex["prediction"] = result["output"]
                # ex["prediction"] = result["results"]
            except Exception as e:
                ex["prediction"] = "ERROR"
                print(f"âš ï¸ Failed to generate prediction: {e}")
                st.markdown(f"âš ï¸ Failed to generate prediction: {e}")

def run_evaluation(qa_set_name="core", sample_size=None):
    print(f"ğŸ“Š Evaluating set: {qa_set_name}")

    examples = qa_sets.get(qa_set_name, [])
    initialize_predictions(examples)

    if not examples:
        print(f"âš ï¸ No examples found for set: {qa_set_name}")
        return []

    if sample_size:
        examples = random.sample(examples, min(sample_size, len(examples)))

    # Evaluate using correct answer vs prediction (for now we mock predictions)
    predictions = [{"query": ex["query"], "result": ex["prediction"]} for ex in examples]
    # predictions = [{"query": ex["query"], "result": ex.get("prediction", ex["answer"])} for ex in examples]
    references = [{"query": ex["query"], "answer": ex["answer"]} for ex in examples]

    graded_outputs = eval_chain.evaluate(examples=references, predictions=predictions)

    # Display results
    for i, result in enumerate(graded_outputs):
        q = examples[i]['query']
        a = examples[i]['answer']
        p = predictions[i]['result']
        g = result.get('results')
        r = result.get('reasoning', 'N/A')
        print(f"\nğŸ”¹ Q: {q}")
        print(f"âœ… A: {a}")
        print(f"ğŸ¤– P: {p}")
        print(f"ğŸ§  Grade: {g}")
        print(f"ğŸ“ Explanation: {r}")
        st.markdown(f"""
        **ğŸ”¹ Q:** {q}  
        **âœ… A:** {a}  
        **ğŸ¤– P:** {p if p else "None"}  
        **ğŸ§  Grade:** `{g}`  
        **ğŸ“ Explanation:** {r}
        """)

    return graded_outputs